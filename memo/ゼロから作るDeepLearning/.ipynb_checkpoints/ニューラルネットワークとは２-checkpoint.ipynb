{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークの学習\n",
    "\n",
    "NNの特徴は、データから学習できる点にある。ここで言う「学習」とは、重みパラメータの値をデータから自動で決定することを指す。機械学習の問題では、**汎化能力**を正しく評価するために訓練データとテストデータの2つに分けて学習する必要がある。（訓練データは教師データと呼ぶ場合もある。）**汎化能力**とは、まだみぬデータに対しての能力であり、この汎化能力を獲得することこそが機械学習の最終的な目標である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "NNの性能の悪さを示す指標。\n",
    "\n",
    "- 二乗和誤差\n",
    "(4.1)\n",
    "$$\n",
    "    E = \\frac{1}{2}\\sum_{k}(y_k-t_k)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y,t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→$y_k$は出力、$t_k$は教師データを表し、$k$はデータの次元数を表す。NNの出力と正解となる教師データの各要素の差の２乗を計算し、その総和を求める。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 交差エントロピー誤差\n",
    "(4.2)\n",
    "$$\n",
    "    E = -\\sum_{k}t_klogy_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return-np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→$y_k$は出力、$t_k$は正解ラベルを表す。$t_k$は正解ラベルとなるインデックスだけが1で、その他は0であるとする。そのため、式(4.2)は正解ラベルが1に対応する出力の自然対数を計算するだけになる。<br>\n",
    "例えば、「2」が正解ラベルのインデックスであるとして、それに対応する出力が0.6の場合、-log0.6=0.51と計算できる。出力が0.1の場合は、-log0.1=2.30となる。式(4.2)は正解ラベルに対応する出力が大きければ大きいほど、0に近く。\n",
    "\n",
    "→np.log(0)のような計算が発生した場合、np.log(0)はマイナスの無限大を表す-infとなり計算を進められない。その防止策として、微小な値であるdeltaを追加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y= [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "cross_entropy_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ミニバッチ学習\n",
    "\n",
    "機械学習の問題は、訓練データが100個ある場合、その100個の損失関数の和を指標としている。そこで、訓練データ全ての損失関数の和を求めたいとすると、交差エントロピー誤差の場合<br>\n",
    "(4.3)\n",
    "$$\n",
    "    E = -\\frac{1}{N}\\sum_n\\sum_kt_{nk}logy_{nk}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→Nはデータ数、$t_{nk}$はn個目のデータのk番目の値を意味する。(4.2)をN個分のデータに拡張した式である。最後にNで割って正規化することで1個あたりの「平均の損失関数」を求めている。そのように平均化すれば、訓練データの数に関係なくいつでも統一した指標が得られる。\n",
    "\n",
    "また、ビッグデータともなれば、その数は数百万、数千万といったオーダーの巨大なデータになる。そこで訓練データからある個数だけを選び出し、その<u>かたまりごとに学習を行う</u>。\n",
    "例えば、60000個の訓練データの中から100個を無作為に選び出して、その100個を使って学習を行う学習方法を**ミニバッチ学習**と言う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ーなぜ損失関数を設定するのか？ー\n",
    "\n",
    "NNの学習における「微分」の役割に注目すると解決する。<br>\n",
    "最適なパラメータを探索する際に、損失関数の値が出来るだけ小さくなるようなパラメータを探す。ここで、出来るだけ小さな損失関数の場所を探すために、パラメータの微分（正確には勾配）を計算し、その微分の値を手がかりにパラメータの値を徐々に更新していく。\n",
    "\n",
    "もし、その微分の値がマイナスとなれば、その重みパラメータを正の方向へ変化させることで、損失関数を減少させることができる。微分の値が0になると、重みパラメータをどちらに動かしても損失関数の値が変わらないため、その重みパラメータの更新はそこでストップする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数値微分\n",
    "\n",
    "(4.4)\n",
    "$$\n",
    "    \\frac{df(x)}{dx} = \\lim_{h \\to 0}\\frac{f(x + h) - f(x)}{h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(4.4)を参考に、関数の微分を求める計算を実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→式(4.4)をそのまま実装すると、誤差がうまれる。「真の微分」はxの位置での関数の傾きに対応するが、(4.4)では(x+h)とxの間の傾きに対応する。そのため厳密には真の微分と一致しない。誤差を減らす工夫として、中心差分近似を用いる。\n",
    "\n",
    "→hを0に無限に近づけようとして、h=10e-50という値を用いると、<u>丸め誤差（小数の小さな範囲において数値が省略されてしまうことで、最終的な計算結果に生じる誤差）</u>が生じてしまう。$10^{-4}$程度の値を用いれば、良い結果が得られることがわかっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 偏微分\n",
    "\n",
    "複数の変数からなる関数の微分を、偏微分という。\n",
    "\n",
    "例）$f(x_{0},x_{1})=x_{0}^{2}+x_{1}^{2},x_{0}=3,x_{1}=4$の時の$x_{0}$に対する偏微分$\\frac{δf}{δx_{0}}$を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配\n",
    "\n",
    "全ての変数の偏微分をベクトルとしてまとめたものを勾配(gradient)という。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        #f(x+h)の計算\n",
    "        x[idx] = tmp_val+h\n",
    "        fxh1 = f(x)\n",
    "        #f(x-h)の計算\n",
    "        x[idx] = tmp_val-h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→np.zeros_like(x)はxと同じ形状の配列で、その要素全てが0の配列を生成するということ。\n",
    "\n",
    "点(3,4)での勾配を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'function_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ed2a60ed2551>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'function_1' is not defined"
     ]
    }
   ],
   "source": [
    "numerical_gradient(function_1, np.array([3.0,4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配法\n",
    "\n",
    "学習の際に最適なパラメータ（重みやバイアス）を見つける必要がある。最適なパラメータというのは、関数が最小値をとるときのパラメータの値のこと。しかし一般的に損失関数は複雑で、どこに最小値があるか見当がつかない。そこで**勾配**をうまく利用して、関数の最小値を探す。<br>\n",
    "**勾配**が示す方向は、各場所において<u>関数の値をもっとも減らす方向</u>。勾配方向へ進むことを繰り返すことで、関数の値を徐々に減らしていく。勾配法を数式で表すと、\n",
    "\n",
    "(4.7)\n",
    "$$\n",
    "    x_{0} = x_{0} - η\\frac{δf}{δx_{0}}, x_{1} = x_{1}-η\\frac{δf}{δx_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ηは**学習率**を表す。1回の学習でどれだけパラメータを更新するか、ということを決める。なお、学習率の値は0.01や0.001など前もって何らかの値に決める必要がある。\n",
    "学習率の値を変更しながら、正しく学習できているかどうか確認作業を行うのが一般的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例）$f(x_{0},x_{1})=x_{0}^2 + x_{1}^2$の最小値を勾配法で求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(f, x, lr, step_num):\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr * grad\n",
    "    return x\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "init_x = np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→init_xは初期値で、lrは学習率、ste_numは勾配法による繰り返しの数とする。最終的な結果は、(-6.1e-10, 8.1e-10)となり、ほとんど(0.0)に近い結果である。\n",
    "\n",
    "学習率は、大きすぎると出力結果は発散してしまい、小さすぎるとほとんど更新されずに終わってしまう。\n",
    "重みやバイアスは、訓練データと学習アルゴリズムによって<u>自動</u>で獲得されるパラメータであるのに対し、学習率は<u>手動で設定されるパラメータ</u>であるので、いろいろな値で試しながらうまく学習できるケースを探す、という作業が必要になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNに対する勾配\n",
    "\n",
    "NNの学習においても勾配（重みパラメータに関する損失関数の勾配）を求める必要がある。\n",
    "\n",
    "(4.8)\n",
    "$$\n",
    "    W =\n",
    "    \\left(\n",
    "    \\begin{matrix}\n",
    "    w_{11} & w_{12} & w_{13} \\\\\n",
    "    w_{21} & w_{22} & w_{23}\n",
    "    \\end{matrix}\n",
    "    \\right)\n",
    "    \\frac{δL}{δW} =\n",
    "    \\left(\n",
    "    \\begin{matrix}\n",
    "    \\frac{δL}{δw_{11}} & \\frac{δL}{δw_{12}} & \\frac{δL}{δw_{13}} \\\\\n",
    "    \\frac{δL}{δw_{21}} & \\frac{δL}{δw_{22}} & \\frac{δL}{δw_{23}}\n",
    "    \\end{matrix}\n",
    "    \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→形状が2×3の重みWだけをもつNNがあり、損失関数をLで表す場合。<br>\n",
    "→1行1列目の要素である$\\frac{δL}{δw_{11}}$は、$w_{11}$を少し変化させると損失関数Lがどれだけ変化するか、ということを表している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に勾配を求める実装を行うために、simpleNetというクラスを実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→np.random.randn()：平均0、標準偏差1の正規分布に従う乱数を返す<br>\n",
    "　np.dot()：Numpyで内積を計算する関数<br>\n",
    "\n",
    "形状が2×3の重みパラメータを一つだけインスタンス変数としてもつ。predict(x)は予測するためのメソッド、loss(x, t)は損失関数を求めるためのメソッドである。<br>\n",
    "ここで引数のxには入力データが、tには正解ラベルが入力されるものとする。\n",
    "\n",
    "このsimpleNetを使ってみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.62006546 -1.59333854 -0.42197585]\n",
      " [ 0.6357717  -0.24165729  0.68248515]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20015525 -1.17349468  0.36105113]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最大値のインデックス\n",
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7260662834310464"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0,0,1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numerical_gradient(f, x)を使って、勾配を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e1d84f40fc2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-dc7d5ab05d66>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtmp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#f(x+h)の計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
