{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習ざっくりまとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習とは\n",
    "機械学習は入力データから反復的に学習を行うことによって、パターンを学習し、新しいデータに対しても答えを得ることができるようにするもの。\n",
    "\n",
    "- 回帰学習：機械学習の問題はデータから数値を予測する\n",
    "- 分類問題：データがどのクラスに属するかを分類する\n",
    "\n",
    "の２つに大別できる。\n",
    "ある問題に対する適切なパラメータを学習していくため、学習するためのデータさえ用意できれば基本的なアルゴリズムを変更することなく、様々な問題を解決していくことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習の手順\n",
    "\n",
    "機械学習では、まず学習モデルの構築を行う。\n",
    "学習モデルは多くの数式を含むモデルである。\n",
    "数式にはパラメータが含まれており、それらのパラメータを決定する「学習」を行った後、学習後のモデルを使って新しい問題を解く「推論」を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「学習」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![画像がありません](./image/train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「推論」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![画像がありません](./image/inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータの学習\n",
    "\n",
    "機械学習の「学習」とは学習モデル内のパラメータを入力データから決定していくことを指す。問題に対して全てのパラメータを適切に設定することは、人間にはほぼ不可能であるため、コンピュータに学習してもらう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数と勾配法\n",
    "\n",
    "**損失関数**は、学習モデルの性能の悪さを評価するための関数。\n",
    "損失関数の結果が0に近いほど、学習モデルは適切な学習ができているということになる。\n",
    "そのため、学習では損失関数の結果が小さくなるようにパラメータを変化させていく。\n",
    "\n",
    "パラメータを変化させるためには**勾配法**という方法を使う。\n",
    "勾配法は損失関数の勾配（微分）が小さくなるようにパラメータを徐々に更新していく。<br>\n",
    "→*微分は関数の変化率なので、微分が0の場所は関数の最小値である可能性があるから。<br>\n",
    "→*微分はパラメータを微小変化させた時の関数の変化量*\n",
    "\n",
    "この時、微分値に対してどの程度パラメータを更新するかという、**学習率**というハイパーパラメータを与える。<br>\n",
    "学習率は大きくても小さくても適切な学習が実行されない。\n",
    "\n",
    "求めるための方法としては、実際にパラメータを微小変化させて結果の差分を計算する数値微分がある。<br>\n",
    "しかし、より効率的で高速な方法として**誤差逆伝播法**（バックプロパゲーション）という方法を使うのが一般的である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差逆伝播法\n",
    "\n",
    "微分はパラメータを変化させた時、最終結果がどの程度変化するかという影響度だと考えることができる。<br>\n",
    "しかし、機械学習のモデルでは数式同士が複雑に影響し合うため、微分を求めることが困難である。\n",
    "\n",
    "そこで、数式を分解し、その最小単位ごとに入力が出力にどれくらい影響を及ぼすかを計算可能な局所的な微分式を求めておく。<br>\n",
    "それを、出力側から逆に遡っていくことで全体の微分を求めることができる。\n",
    "出力から入力まで逆方向に計算を辿っていくので、逆伝播という。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![画像がありません](./image/backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図の黒字が順方向の計算であり、赤字が逆方向の計算（＝各項の影響度）。<br>\n",
    "例えば、100の項が1増えて101になった場合、最終結果は604になるため、100の項の影響度は4になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習モデル\n",
    "### パーセプトロン\n",
    "\n",
    "パーセプトロンは入力とパラメータの積和が閾値以下であれば0、閾値より上であれば1を出力する単純なモデルである。<br>\n",
    "多くの場合、閾値θをバイアス-bと置き換え、計算結果が0以下か、0より上かで出力を変化させる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![画像がありません](./image/node1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パーセプトロンが一つだけでは単純なモデルしか表現できないため、複雑なモデルを表現するためにはパーセプトロンを複数層重ねる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![画像がありません](./image/node2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワーク\n",
    "\n",
    "ニューラルネットワーク（以下、NN）の構造自体は、パーセプトロンの非常に似ている。NNとパーセプトロンの一番の違いは**活性化関数**である。活性化関数hは入力信号とパラメータの積和の結果aを次の入力に変換する関数のこと。\n",
    "![画像がありません](./image/ActivationFunction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パーセプトロンでは、積和の結果が0以上の場合に１、それ以外の場合に０だったので、活性化関数に**ステップ関数**を用いている事になる。<br>\n",
    "NNでは、**シグモイド関数**や**ReLU関数**といった微分可能な関数を使うことで、より高度な学習が可能となる。<br>\n",
    "\n",
    "**出力層の活性化関数だけはソフトマックス関数**など、<u>中間層とは異なる関数を使う。</u><br>\n",
    "ソフトマックス関数は、各出力結果を百分率に直すような関数。分類問題においては、出力層の各ノードが入力が属するクラスに対応しているため、ソフトマックス関数の各結果はそのクラスに属している確率を表す。<br>\n",
    "\n",
    "しかし、分類問題の場合は一番確率の高いクラスだけ分かれば良いという場合が多いので、ソフトマックス関数を使わず出力層の結果で一番大きなクラスを取り出すという処理を行う場合もある。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「活性化関数」<br>\n",
    "　学習のタイプが<u>分類</u>ならば、**ソフトマックス関数**を用いる。<br>\n",
    " <u>回帰</u>ならば、**恒等写像**を用いる。\n",
    " \n",
    " 「誤差関数」<br>\n",
    " <u>分類</u>ならば、**交差エントロピー**を用い、<u>回帰</u>ならば**二乗誤差**を用いる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畳み込みニューラルネットワーク\n",
    "\n",
    "NNでは、隣接する層の入力が全て結合する「全結合」(aを求める処理）と、その結果を次の層の入力に変換する「活性化」の、２種類の層が重なり、最後に「ソフトマックス」関数を使って最終的な出力を得る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![画像がありません](./image/cnn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みニューラルネットワークでは、「畳み込み」層と「プーリング」層が加わる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![画像がありません](./image/cnn2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
